{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65546e2b-63c2-49a9-97e0-8e64461c4c6f",
   "metadata": {},
   "source": [
    "# install datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025b258-35ce-4a26-8ee8-876636bb2a19",
   "metadata": {},
   "source": [
    "If you're running it on Colab, you'll need to pip install a few libraries: datasets, bitsandbytes, and trl. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb9a8fa-3dea-4d42-b569-58f045b58a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.56.1 peft==0.17.0 accelerate==1.10.0 trl==0.23.1 bitsandbytes==0.47.0 datasets==4.0.0 huggingface-hub==0.34.4 safetensors==0.6.2 pandas==2.2.2 matplotlib==3.10.0 numpy==2.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26ebbd0d-7e76-4604-8cf9-fef83266ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f2094-e1e9-45cc-a0c5-9b3d97f759ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float32\n",
    ")\n",
    "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "   repo_id, device_map=\"cuda:0\", quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfba089-0196-4ee0-b7b1-324534524eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356ab581-83e6-4b57-8abe-5a9b28033ab1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd609baa-2e48-4011-b4bd-28a770148755",
   "metadata": {},
   "source": [
    "# Setting Up Low-Rank Adapters (LoRA)\n",
    "\n",
    "Low-rank adapters can be attached to each and every one of the quantized layers. The adapters are mostly regular Linear layers that can be easily updated as usual. The clever trick in this case is that these adapters are significantly smaller than the layers that have been quantized.\n",
    "\n",
    "Since the quantized layers are frozen (they cannot be updated), setting up LoRA adapters on a quantized model drastically reduces the total number of trainable parameters to just 1% (or less) of its original size.\n",
    "\n",
    "We can set up LoRA adapters in three easy steps:\n",
    "\n",
    "    Call prepare_model_for_kbit_training() to improve numerical stability during training.\n",
    "    Create an instance of LoraConfig.\n",
    "    Apply the configuration to the quantized base model using the get_peft_model() method.\n",
    "\n",
    "Let's try it out with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903bd50-e4aa-4621-aae8-f1d396368fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    # the rank of the adapter, the lower the fewer parameters you'll need to train\n",
    "    r=8,                   \n",
    "    lora_alpha=16, # multiplier, usually 2*r\n",
    "    bias=\"none\",           \n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Newer models, such as Phi-3 at time of writing, may require \n",
    "    # manually setting target modules\n",
    "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d9435-c065-46d7-96c4-b56a946a0850",
   "metadata": {},
   "source": [
    "The quantized layers (Linear4bit) have turned into lora.Linear4bit modules where the quantized layer itself became the base_layer with some regular Linear layers (lora_A and lora_B) added to the mix.\n",
    "\n",
    "These extra layers would make the model only slightly larger. However, the model preparation function (prepare_model_for_kbit_training()) turned every non-quantized layer to full precision (FP32), thus resulting in a 30% larger model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b576e1e4-d008-4857-9966-54003c35984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1818ceba-535d-4460-b0ac-6b59784499a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p, tot_p = model.get_nb_trainable_parameters()\n",
    "print(f'Trainable parameters:      {train_p/1e6:.2f}M')\n",
    "print(f'Total parameters:          {tot_p/1e6:.2f}M')\n",
    "print(f'% of trainable parameters: {100*train_p/tot_p:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfc206-68d3-43e9-8459-3d9a4e0747ea",
   "metadata": {},
   "source": [
    "The model is ready to be fine-tuned, but we are still missing one key component: our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd46c5f-3c04-46f6-9261-55771ff85be7",
   "metadata": {},
   "source": [
    "The dataset yoda_sentences consists of 720 sentences translated from English to Yoda-speak. The dataset is hosted on the Hugging Face Hub and we can easily load it using the load_dataset() method from the Hugging Face datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1ea86-f599-4dec-bdd8-e5718e5dd7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503cfc6-bc0d-49ae-ae6c-2e25cc103772",
   "metadata": {},
   "source": [
    "# The dataset has three columns:\n",
    "\n",
    "    original English sentence (sentence)\n",
    "    basic translation to Yoda-speak (translation)\n",
    "    enhanced translation including typical Yesss and Hrrmm interjections (translation_extra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a35f99-2e89-44e0-808d-32eaa96b0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e5d86-8912-40a9-b208-aaa4583e2ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from trl.extras.dataset_formatting.instructions_formatting_function\n",
    "# Converts dataset from prompt/completion format (not supported anymore)\n",
    "# to the conversational format\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(converted_sample)\n",
    "        return {'messages': output_texts}\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return {'messages': converted_sample}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af1aa7-613a-4e48-864b-52ef33fbd984",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.map(format_dataset)\n",
    "dataset = dataset.remove_columns(['prompt', 'completion', 'translation'])\n",
    "messages = dataset[0]['messages']\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50629ca-3770-497b-84a8-943a98207c18",
   "metadata": {},
   "source": [
    "# Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f22e24-d092-408f-b25e-30d2a8dc2206",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.pad_token_id = tokenizer.unk_token_id\n",
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591ed41-fff7-47ab-b33a-91fabae7d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.apply_chat_template(messages, tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14e021b-adda-49f5-aaaf-3f20dc9cebd2",
   "metadata": {},
   "source": [
    "# SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc5a17-6a35-4f01-a4a3-5bf50c7d1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    ## GROUP 1: Memory usage\n",
    "    # These arguments will squeeze the most out of your GPU's RAM\n",
    "    # Checkpointing\n",
    "    gradient_checkpointing=True,    # this saves a LOT of memory\n",
    "    # Set this to avoid exceptions in newer versions of PyTorch\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False}, \n",
    "    # Gradient Accumulation / Batch size\n",
    "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
    "    gradient_accumulation_steps=1,  \n",
    "    # The initial (micro) batch size to start off with\n",
    "    per_device_train_batch_size=16, \n",
    "    # If batch size would cause OOM, halves its size until it works\n",
    "    auto_find_batch_size=True,\n",
    "\n",
    "    ## GROUP 2: Dataset-related\n",
    "    max_length=64, # renamed in v0.20\n",
    "    # Dataset\n",
    "    # packing a dataset means no padding is needed\n",
    "    packing=True,\n",
    "    packing_strategy='wrapped', # added to approximate original packing behavior\n",
    "\n",
    "    ## GROUP 3: These are typical training parameters\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    # Optimizer\n",
    "    # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n",
    "    optim='paged_adamw_8bit',       \n",
    "    \n",
    "    ## GROUP 4: Logging parameters\n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./phi3-mini-yoda-adapter',\n",
    "    report_to='none'.\n",
    "\n",
    "    # ensures bf16 (the new default) is only used when it is actually available\n",
    "    bf16=torch.cuda.is_bf16_supported(including_emulation=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b31e45-9f1e-4776-9899-4cd51056dd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model.base_model.model, # the underlying Phi-3 model\n",
    "    peft_config=config,  # added to fix issue in TRL>=0.20\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4bbe2e-7a11-47a8-9012-a5cb6728cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e166271e-790c-49e0-8c0a-8fbec940c14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['input_ids'][0], batch['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72423eb3-c9a4-42b5-b889-602f03474303",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab0cf6-25b2-4c77-bc65-300c761fe1fc",
   "metadata": {},
   "source": [
    "# Querying the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0e122-df42-40fd-8439-8afb11f7cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [{\"role\": \"user\", \"content\": sentence}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        converted_sample, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83302a6b-0809-4ea6-9d04-a315d7552003",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The Force is strong in you!'\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc4ff61-82ac-4f7f-8c0a-822342f13bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import nullcontext\n",
    "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
    "    tokenized_input = tokenizer(\n",
    "        prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    # if it was trained using mixed precision, uses autocast context\n",
    "    ctx = torch.autocast(device_type=model.device.type, dtype=model.dtype) \\\n",
    "          if model.dtype in [torch.float16, torch.bfloat16] else nullcontext()\n",
    "    with ctx:  \n",
    "        gen_output = model.generate(**tokenized_input,\n",
    "                                    eos_token_id=tokenizer.eos_token_id,\n",
    "                                    max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8ab146-156a-4537-b525-3fd6c7016411",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate(model, tokenizer, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e5f30c-92f5-453b-8448-f0999bd54dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('local-phi3-mini-yoda-adapter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
