{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65546e2b-63c2-49a9-97e0-8e64461c4c6f",
   "metadata": {},
   "source": [
    "# install datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9025b258-35ce-4a26-8ee8-876636bb2a19",
   "metadata": {},
   "source": [
    "If you're running it on Colab, you'll need to pip install a few libraries: datasets, bitsandbytes, and trl. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb9a8fa-3dea-4d42-b569-58f045b58a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.56.1 peft==0.17.0 accelerate==1.10.0 trl==0.23.1 bitsandbytes==0.47.0 datasets==4.0.0 huggingface-hub==0.34.4 safetensors==0.6.2 pandas==2.2.2 matplotlib==3.10.0 numpy==2.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26ebbd0d-7e76-4604-8cf9-fef83266ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f2094-e1e9-45cc-a0c5-9b3d97f759ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_use_double_quant=True,\n",
    "   bnb_4bit_compute_dtype=torch.float32\n",
    ")\n",
    "repo_id = 'microsoft/Phi-3-mini-4k-instruct'\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "   repo_id, device_map=\"cuda:0\", quantization_config=bnb_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfba089-0196-4ee0-b7b1-324534524eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "356ab581-83e6-4b57-8abe-5a9b28033ab1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd609baa-2e48-4011-b4bd-28a770148755",
   "metadata": {},
   "source": [
    "# Setting Up Low-Rank Adapters (LoRA)\n",
    "\n",
    "Low-rank adapters can be attached to each and every one of the quantized layers. The adapters are mostly regular Linear layers that can be easily updated as usual. The clever trick in this case is that these adapters are significantly smaller than the layers that have been quantized.\n",
    "\n",
    "Since the quantized layers are frozen (they cannot be updated), setting up LoRA adapters on a quantized model drastically reduces the total number of trainable parameters to just 1% (or less) of its original size.\n",
    "\n",
    "We can set up LoRA adapters in three easy steps:\n",
    "\n",
    "    Call prepare_model_for_kbit_training() to improve numerical stability during training.\n",
    "    Create an instance of LoraConfig.\n",
    "    Apply the configuration to the quantized base model using the get_peft_model() method.\n",
    "\n",
    "Let's try it out with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903bd50-e4aa-4621-aae8-f1d396368fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    # the rank of the adapter, the lower the fewer parameters you'll need to train\n",
    "    r=8,                   \n",
    "    lora_alpha=16, # multiplier, usually 2*r\n",
    "    bias=\"none\",           \n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Newer models, such as Phi-3 at time of writing, may require \n",
    "    # manually setting target modules\n",
    "    target_modules=['o_proj', 'qkv_proj', 'gate_up_proj', 'down_proj'],\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d9435-c065-46d7-96c4-b56a946a0850",
   "metadata": {},
   "source": [
    "The quantized layers (Linear4bit) have turned into lora.Linear4bit modules where the quantized layer itself became the base_layer with some regular Linear layers (lora_A and lora_B) added to the mix.\n",
    "\n",
    "These extra layers would make the model only slightly larger. However, the model preparation function (prepare_model_for_kbit_training()) turned every non-quantized layer to full precision (FP32), thus resulting in a 30% larger model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b576e1e4-d008-4857-9966-54003c35984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1818ceba-535d-4460-b0ac-6b59784499a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_p, tot_p = model.get_nb_trainable_parameters()\n",
    "print(f'Trainable parameters:      {train_p/1e6:.2f}M')\n",
    "print(f'Total parameters:          {tot_p/1e6:.2f}M')\n",
    "print(f'% of trainable parameters: {100*train_p/tot_p:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfc206-68d3-43e9-8459-3d9a4e0747ea",
   "metadata": {},
   "source": [
    "The model is ready to be fine-tuned, but we are still missing one key component: our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd46c5f-3c04-46f6-9261-55771ff85be7",
   "metadata": {},
   "source": [
    "The dataset yoda_sentences consists of 720 sentences translated from English to Yoda-speak. The dataset is hosted on the Hugging Face Hub and we can easily load it using the load_dataset() method from the Hugging Face datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1ea86-f599-4dec-bdd8-e5718e5dd7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9e449-7481-4c9b-bb40-4e45b7687221",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset({\n",
    "features: ['sentence', 'translation', 'translation_extra'],\n",
    "num_rows: 720\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503cfc6-bc0d-49ae-ae6c-2e25cc103772",
   "metadata": {},
   "source": [
    "# The dataset has three columns:\n",
    "\n",
    "    original English sentence (sentence)\n",
    "    basic translation to Yoda-speak (translation)\n",
    "    enhanced translation including typical Yesss and Hrrmm interjections (translation_extra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a35f99-2e89-44e0-808d-32eaa96b0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e5d86-8912-40a9-b208-aaa4583e2ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af1aa7-613a-4e48-864b-52ef33fbd984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50629ca-3770-497b-84a8-943a98207c18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f22e24-d092-408f-b25e-30d2a8dc2206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591ed41-fff7-47ab-b33a-91fabae7d624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
